{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('GBTree').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data_processed/mp_data_24hr.csv', header = True, inferSchema = True)\n",
    "df = df.withColumnRenamed(\"hospital_expire_flag\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntegerType 24\n",
      "StringType 4\n",
      "DoubleType 103\n"
     ]
    }
   ],
   "source": [
    "# df.printSchema()\n",
    "from collections import defaultdict\n",
    "data_types = defaultdict(list)\n",
    "for field in df.schema.fields:\n",
    "    data_types[str(field.dataType)].append(field.name)\n",
    "for key in data_types.keys():\n",
    "    print(key, len(data_types[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category Indexing, One-Hot Encoding, Imputer, and VectorAssembler - a feature transformer that merges multiple columns into a vector column.\n",
    "\n",
    "The code below indexes each categorical column using the StringIndexer, then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row. VectorAssembler combines all the feature columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "# categorical features\n",
    "categoricalColumns = ['ethnicity','gender','admission_type']\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index').setHandleInvalid(\"keep\") # keep unseen labels\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# numerical features    \n",
    "numericCols = ['age', 'icustay_num',\n",
    "               'heartrate_mean', 'sysbp_mean', 'diasbp_mean', 'meanbp_mean',\n",
    "               'resprate_mean', 'tempc_mean', 'spo2_mean', 'glucose_mean', \n",
    "               'heartrate_min', 'sysbp_min', 'diasbp_min','meanbp_min', \n",
    "               'resprate_min', 'tempc_min', 'spo2_min', 'glucose_min',\n",
    "               'heartrate_max', 'sysbp_max', 'diasbp_max', 'meanbp_max',\n",
    "               'resprate_max', 'tempc_max', 'spo2_max', 'glucose_max', \n",
    "               'gcs_mean', 'gcsmotor_mean', 'gcsverbal_mean', 'gcseyes_mean', 'endotrachflag_mean',\n",
    "               'gcs_min', 'gcsmotor_min', 'gcsverbal_min', 'gcseyes_min', 'endotrachflag_min', \n",
    "               'gcs_max', 'gcsmotor_max', 'gcsverbal_max', 'gcseyes_max', 'endotrachflag_max', \n",
    "               'baseexcess_mean', 'carboxyhemoglobin_mean', 'methemoglobin_mean', \n",
    "               'po2_mean', 'pco2_mean', 'ph_mean', 'pao2fio2ratio_mean', 'totalco2_mean', \n",
    "               'aniongap_mean', 'albumin_mean', 'bands_mean', 'bicarbonate_mean', \n",
    "               'bilirubin_mean', 'calcium_mean', 'creatinine_mean', 'chloride_mean', \n",
    "               'hematocrit_mean', 'hemoglobin_mean', 'lactate_mean', 'platelet_mean', \n",
    "               'potassium_mean', 'ptt_mean', 'inr_mean', 'sodium_mean', 'bun_mean', 'wbc_mean',\n",
    "               'baseexcess_min', 'carboxyhemoglobin_min', 'methemoglobin_min',\n",
    "               'po2_min', 'pco2_min', 'ph_min', 'pao2fio2ratio_min', 'totalco2_min',\n",
    "               'aniongap_min', 'albumin_min', 'bands_min', 'bicarbonate_min',\n",
    "               'bilirubin_min', 'calcium_min', 'creatinine_min', 'chloride_min',\n",
    "               'hematocrit_min', 'hemoglobin_min', 'lactate_min', 'platelet_min',\n",
    "               'potassium_min', 'ptt_min', 'inr_min', 'sodium_min', 'bun_min', 'wbc_min', \n",
    "               'baseexcess_max', 'carboxyhemoglobin_max', 'methemoglobin_max', \n",
    "               'po2_max', 'pco2_max', 'ph_max', 'pao2fio2ratio_max', 'totalco2_max',          \n",
    "               'aniongap_max', 'albumin_max', 'bands_max', 'bicarbonate_max', \n",
    "               'bilirubin_max', 'calcium_max', 'creatinine_max', 'chloride_max', \n",
    "               'hematocrit_max', 'hemoglobin_max', 'lactate_max', 'platelet_max', \n",
    "               'potassium_max', 'ptt_max', 'inr_max', 'sodium_max', 'bun_max', 'wbc_max', \n",
    "               'urineoutput']\n",
    "\n",
    "imputer = Imputer(strategy='median', inputCols=numericCols, outputCols=['imputed_' + col for col in numericCols])\n",
    "stages += [imputer]\n",
    "\n",
    "# assemble\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + ['imputed_' + col for col in numericCols]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntegerType 5\n",
      "StringType 4\n",
      "DoubleType 122\n"
     ]
    }
   ],
   "source": [
    "# categorical columns fill na\n",
    "df = df.fillna('missing', subset=categoricalColumns)\n",
    "# cast integer columns to double datatype for the imputer\n",
    "for col in numericCols:\n",
    "    if col in data_types['IntegerType']:\n",
    "        df = df.withColumn(col, df[col].cast('double'))\n",
    "# check\n",
    "data_types = defaultdict(list)\n",
    "for field in df.schema.fields:\n",
    "    data_types[str(field.dataType)].append(field.name)\n",
    "for key in data_types.keys():\n",
    "    print(key, len(data_types[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow. A Pipelineâ€™s stages are specified as an ordered array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(train)\n",
    "train = pipelineModel.transform(train)\n",
    "test = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selectedCols = ['label', 'features']\n",
    "train = train.select(selectedCols)\n",
    "test = test.select(selectedCols)\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|       rawPrediction|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "|[-0.9572764035626...|       1.0|[0.12847023138874...|\n",
      "|[1.20082617098612...|       0.0|[0.91695321609176...|\n",
      "|[0.62617834106363...|       0.0|[0.77770754751751...|\n",
      "|[1.11227174253371...|       0.0|[0.90243197488551...|\n",
      "|[1.02779021105879...|       0.0|[0.88651027603981...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'label', maxIter=10, seed=0)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select('rawPrediction', 'prediction', 'probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8293142966764953\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(\"Test Area Under ROC: \" + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to tune Gradient Boosting Tree model with the ParamGridBuilder and the CrossValidator. Before that we can use explainParams() to print a list of all params and their definitions to understand what params available for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 10)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 3504127614838123891, current: 0)\n",
      "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(gbt.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8620531632896719"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxIter, [20, 50])\n",
    "             .addGrid(gbt.maxDepth, [3, 5, 10])\n",
    "             .addGrid(gbt.minInstancesPerNode, [3])\n",
    "             .addGrid(gbt.subsamplingRate, [1, 0.8])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations.\n",
    "cvModel = cv.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrossValidatorModel' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-93ee1d63a6cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainingSummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FPR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TPR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ROC curve (area = %0.2f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrainingSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mareaUnderROC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossValidatorModel' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "trainingSummary = cvModel.summary\n",
    "\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "lw = 2\n",
    "plt.plot(roc['FPR'],roc['TPR'], color='red', lw=lw, label='ROC curve (area = %0.2f)' % trainingSummary.areaUnderROC)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC (24-hour data)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "fig.savefig('../img/GBT.png')\n",
    "plt.show()\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
